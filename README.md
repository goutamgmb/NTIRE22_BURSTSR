## Table of contents 
* [Introduction](#introduction)
* [Reference](#reference)
* [Dates](#dates)
* [Description](#description)
* [Track 1 - Synthetic](#track-1---synthetic)
* [Track 2 - Real-world](#track-2---real-world)
* [Toolkit](#toolkit)
* [Issues and Questions](#issues-and-questions)
* [Organizers](#organizers)
* [Terms and conditions](#terms-and-conditions)
* [Acknowledgements](#acknowledgements)

## Introduction
Burst Image Super-Resolution Challenge will be held as part of the [7th edition of 
NTIRE: New Trends in Image Restoration and Enhancement](https://data.vision.ee.ethz.ch/cvl/ntire22/) workshop to be held in conjunction 
with [CVPR 2022](http://cvpr2022.thecvf.com/). The task of this challenge is to generate 
a denoised, demosaicked, higher-resolution image, given a RAW burst as input. 
The challenge has 2 tracks, namely **Track 1: Synthetic** and 
**Track 2: Real-world**. The top ranked participants in each track will be awarded and all 
participants are invited to submit a paper describing their solution to the associated 
NTIRE workshop at CVPR 2022


## Dates
* 2022.01.31 Release of train and validation data  
* 2022.02.01 Validation server online  
* 2022.03.23 Final test data release (inputs only)  
* 2022.03.30 Test output results submission deadline  
* 2022.03.30 Fact sheets and code/executable submission deadline  
* 2022.04.01 Preliminary test results released to the participants  
* 2022.04.11 Paper submission deadline for entries from the challenge  
* 2022.06.19 NTIRE workshop and challenges, results and award ceremony  


## Description
Given multiple noisy RAW images of a scene, the task in burst super-resolution is to 
predict a denoised higher-resolution RGB image by combining information from the 
multiple input frames. Concretely, the method will be provided a burst sequence 
containing 14 images, where each image contains the RAW sensor data from a bayer filter 
(RGGB) mosaic. The images in the burst have unknown offsets with respect to each other, 
and are corrupted by noise. The goal is to exploit the information from the multiple 
input images to predict a denoised, demosaicked RGB image having a 4 times higher 
resolution, compared to the input. The challenge will have two tracks, 
namely 1) Synthetic and 2) Real-world based on the source of the input data. The goal 
in both the tracks is to reconstruct the **original** image as well as possible, and 
not to artificially generate a plausible, visually pleasing image.





## Track 1 - Synthetic
In the synthetic track, the input bursts are generated from RGB images using a synthetic 
data generation pipeline. 

**Data generation:** The input sRGB image is first converted to linear sensor space 
using an inverse camera pipeline. A LR burst is then generated by applying random 
translations and rotations, followed by bilinear downsampling. The generated burst is 
then mosaicked and corrupted by random noise. 

**Training set:** We provide [code](datasets/synthetic_burst_train_set.py) to generate the synthetic 
bursts using any image dataset for training. Note that any image dataset **except the 
validation split of the [BurstSR dataset](https://openaccess.thecvf.com/content/CVPR2021/papers/Bhat_Deep_Burst_Super-Resolution_CVPR_2021_paper.pdf)** can be used to generate synthetic bursts for training.  

**Validation set:** The bursts in the validation set have been 
pre-generated with the [data generation code](datasets/synthetic_burst_train_set.py), 
using the RGB images from the validation split of the 
[BurstSR dataset](https://openaccess.thecvf.com/content/CVPR2021/papers/Bhat_Deep_Burst_Super-Resolution_CVPR_2021_paper.pdf). **NOTE:** Compared to the validation set from the 2021 version of the challenge, the validation set this year contains bursts 
of higher spatial resolution as input (256x256 instead of 96x96).

### Registration
If you wish to participate in the Synthetic track, please register for the challenge at the 
[codalab page](https://codalab.lisn.upsaclay.fr/competitions/1750#learn_the_details) to get access to the evaluation server and receive email notifications for 
the challenge.

### Evaluation
The methods will be ranked using the fidelity (in terms of PSNR) with the high-resolution 
ground truth, i.e. the linear sensor space image used to generate the burst. The focus of 
the challenge is on learning to reconstruct the original high-resolution image, and not 
the subsequent post-processing. Hence, the PSNR computation will be computed in the 
linear sensor space, before post-processing steps such as color correction, 
white-balancing, gamma correction etc.


### Validation

The results on the **validation set** can be uploaded on the [Codalab server](https://codalab.lisn.upsaclay.fr/competitions/1750#participate) (**live now**)
to obtain the performance measures, as well as a live leaderboard ranking. The results should be uploaded as a ZIP file
containing the network predictions for each burst. The predictions must be normalized to the range [0, 2^14] and saved
as 16 bit (uint16) png files. Please refer to [save_results_synburst_val.py](scripts/save_results_synburst_val.py) for
an example on how to save the results. An example submission file is available [here](https://data.vision.ee.ethz.ch/bhatg/syn_burst_example_submission_2022.zip).

### Final Submission
The **test set** is now public. You can download the test set containing 92 synthetic bursts from [this link](https://data.vision.ee.ethz.ch/bhatg/track1_2022_test_set.zip). You can use the dataset class provided in [synthetic_burst_test_set.py](datasets/synthetic_burst_test_set.py) in the latest commit to load the burst sequences.

For the final submission, you need to submit:
* The predicted outputs for each burst sequence as a zip folder, in the same format as used for uploading results to the codalab validation server (see [this](https://github.com/goutamgmb/NTIRE22_BURSTSR#validation) for details).
* The code and model files necessary to reproduce your results.
* A factsheet (both PDF and tex files) describing your method. The template for the factsheet is available [here](https://data.vision.ee.ethz.ch/bhatg/NTIRE_BURSTSR_TEMPLATE.zip).  

The results, code, and factsheet should be submitted via the [google form](https://docs.google.com/forms/d/e/1FAIpQLSduZNcb6M-e_ROEnATRJ7e58ChUrLgrQ7iSmS6ysoON3wHZqg/viewform?usp=sf_link)

**NOTE:** Training on the validation split is **NOT** allowed for test set submissions.

## Track 2 - Real-world
This track deals with the problem of real-world burst super-resolution. Methods will be evaluated on a test set containing 
bursts captured from a handheld Samsung Galaxy S8 smartphone camera. 

**Training set:** Participants are allowed to use any dataset for training, except the validation and test splits of the 
[BurstSR dataset](https://openaccess.thecvf.com/content/CVPR2021/papers/Bhat_Deep_Burst_Super-Resolution_CVPR_2021_paper.pdf).
Specifically, the participants can use the training split of [BurstSR dataset](https://openaccess.thecvf.com/content/CVPR2021/papers/Bhat_Deep_Burst_Super-Resolution_CVPR_2021_paper.pdf). BurstSR dataset contains RAW bursts captured from a handheld Samsung Galaxy S8 smartphone camera. For each burst, 
a corresponding high-resolution RGB image captured using a DSLR is also provided. The participants are allowed to use either 
the pre-processed version of the dataset containing 160x160 crops, or the unprocessed dataset containing full-sized bursts. 
Additionally, the participants are encouraged to develop unsupervised training methods, or training pipelines using synthetic data.

### Registration
If you wish to participate in the Real-world track, please register for the challenge at the 
[codalab page](https://codalab.lisn.upsaclay.fr/competitions/1751#participate) to receive email notifications for 
the challenge.

### Evaluation
The methods will be evaluated based on user study on a test set  containing bursts captured from a handheld Samsung Galaxy S8 smartphone camera, 
i.e. the same camera which was used to collect the BurstSR dataset. 
The test set will contain bursts containing 14 RAW images, each of spatial size 256x256. The emphasis of the user study 
will be on which method can best reconstruct the **original** high-frequency details. The 
goal is thus not to generate more pleasing images by modifying the output color space 
or generating artificial high frequency content not existing in the high-resolution 
ground truth.

**NOTE:** Unlike in 2021 version of the challenge, we will not use the AlignedPSNR metric to rank the methods this year. 
This is because the AlignedPSNR metric is biased to prefer methods trained use the AlignedL2 loss. Thus, in order to 
encourage development of alternate training strategies for real data, we will rank the methods using only a user study.



### Validation

The will be no evaluation server for Track 2. The participants can use the bursts from the validation split of BurstSR 
dataset for validating their methods. 



### Final Submission
The **test set** is now public. You can download the test set containing 20 real-world bursts from 
[this link](https://data.vision.ee.ethz.ch/bhatg/track2_2022_test_set.zip). You can use the dataset class provided in 
[realworld_burst_test_set.py](datasets/realworld_burst_test_set.py) in the latest commit to load the burst sequences.

For the final submission, you need to submit:
* The predicted outputs for each burst sequence as a zip folder, in the same format as used for uploading results to the codalab validation server (see [this](https://github.com/goutamgmb/NTIRE22_BURSTSR#validation) for details).
* The code and model files necessary to reproduce your results.
* A factsheet (both PDF and tex files) describing your method. The template for the factsheet is available [here](https://data.vision.ee.ethz.ch/bhatg/NTIRE_BURSTSR_TEMPLATE.zip).  

The results, code, and factsheet should be submitted via the [google form](https://docs.google.com/forms/d/e/1FAIpQLSduZNcb6M-e_ROEnATRJ7e58ChUrLgrQ7iSmS6ysoON3wHZqg/viewform?usp=sf_link)


## Toolkit
We also provide a Python toolkit which includes the necessary data loading and 
evaluation scripts. The toolkit contains the following modules.

* [data_processing](data_processing): Contains the forward and inverse camera pipeline 
  employed in [“Unprocessing images for learned raw denoising”](https://arxiv.org/abs/1811.11127), 
  as well as the [script](data_processing/synthetic_burst_generation.py) to generate a 
  synthetic burst from a single RGB image.
* [datasets](datasets): Contains the PyTorch dataset classes useful for the challenge. 
    * [synthetic_burst_train_set](datasets/synthetic_burst_train_set.py) provides the SyntheticBurst dataset which generates synthetic bursts using any image dataset. 
    * [zurich_raw2rgb_dataset](datasets/zurich_raw2rgb_dataset.py) can be used to load 
      the RGB images Zurich RAW to RGB mapping dataset. This can be used along with SyntheticBurst dataset to generate synthetic bursts for training.  	
    * [synthetic_burst_val_set](datasets/synthetic_burst_val_set.py) can be used to load 
      the pre-generated synthetic validation set.
    * [synthetic_burst_test_set](datasets/synthetic_burst_test_set.py) can be used to load 
      the pre-generated synthetic test set.
    * [realworld_burst_test_set](datasets/realworld_burst_test_set.py) can be used to load 
      the real world bursts for track 2 test set.
    * [burstsr_dataset](datasets/burstsr_dataset.py) provides the BurstSRDataset class which can be used to load the RAW bursts and high-resolution ground truths
   from the pre-processed BurstSR dataset.
* [scripts](scripts): Includes useful example scripts.
    * [download_burstsr_dataset](scripts/download_burstsr_dataset.py) can be used to 
      download and unpack the BurstSR dataset.
    * [download_raw_burstsr_data](scripts/download_raw_burstsr_data.py) can be used to download the unprocessed BurstSR dataset.
    * [test_synthetic_burst](scripts/test_synthetic_bursts.py) provides an example on how
  to use the [SyntheticBurst](datasets/synthetic_burst_train_set.py) dataset.
    * [test_burstsr_dataset](scripts/test_burstsr_dataset.py) provides an example on how
  to use the pre-processed [BurstSR](datasets/burstsr_dataset.py) dataset.
    * [save_results_synburst_val](scripts/save_results_synburst_val.py) provides an example
      on how to save the results on [SyntheticBurstVal](datasets/synthetic_burst_val_set.py) 
      dataset for submission on the evaluation server.
    * [save_results_synburst_test](scripts/save_results_synburst_test.py) provides an example
      on how to save the results on [SyntheticBurstTest](datasets/synthetic_burst_test_set.py) 
      dataset for the final submission.
    * [save_results_realworld_test](scripts/save_results_realworld_test.py) provides an example
      on how to save the results on [RealWorldBurstTest](datasets/realworld_burst_test_set.py) 
      dataset for the final submission.
    * [visualize_synburst_results](scripts/visualize_synburst_results.py) Visualize generated results on the synthetic burst 
  validation set.
    
* [utils](utils): Contains utility functions.

**Installation:** The toolkit requires [PyTorch](https://pytorch.org/) and [OpenCV](https://opencv.org/) 
for track 1, and additionally [exifread](https://pypi.org/project/ExifRead/) for track 2. The necessary packages can be installed with 
[anaconda](https://www.anaconda.com/), using the [install.sh](install.sh) script. 


## Data
We provide the following data as part of the challenge. 

**Synthetic validation set:** The official validation set for track 1. The dataset contains 100 synthetic bursts, each containing 
14 RAW images of 256x256 resolution. The synthetic bursts are generated from the RGB Canon images from the validation split of the BurstSR dataset. 
The dataset can be downloaded from [here](https://data.vision.ee.ethz.ch/bhatg/synburst_val_2022.zip).

**Synthetic test set:** The official test set for track 1. The dataset contains 92 synthetic bursts, each containing 
14 RAW images of 256x256 resolution. The synthetic bursts are generated from the RGB Canon images from the test split of the BurstSR dataset. 
The dataset can be downloaded from [here](https://data.vision.ee.ethz.ch/bhatg/synburst_test_2022.zip).

**Real world test set:** The official test set for track 2. The dataset contains 20 real world bursts, 
each containing 14 RAW images of 256x256 resolution. The bursts are captured using a Samsung Galaxy S8 smartphone camera.
The dataset can be downloaded from [here](https://data.vision.ee.ethz.ch/bhatg/realworld_test_2022.zip).

**BurstSR train and validation set (pre-processed):** The dataset has been split into 10 parts and can be downloaded and unpacked using the 
[download_burstsr_dataset.py](scripts/download_burstsr_dataset.py) script. In case of issues with the script, the download links 
are available [here](burstsr_links.md).

**BurstSR train and validation set (raw):** The dataset can be downloaded and unpacked using the [scripts/download_raw_burstsr_data.py](scripts/download_raw_burstsr_data.py) script.

**Zurich RAW to RGB mapping set:** The RGB images from the training split of the 
[Zurich RAW to RGB mapping dataset](http://people.ee.ethz.ch/~ihnatova/pynet.html#dataset) 
can be downloaded from [here](https://data.vision.ee.ethz.ch/bhatg/zurich-raw-to-rgb.zip). These RGB images can be 
used to generate synthetic bursts for training using  the SyntheticBurst class.

## Issues and questions: 
In case of any questions about the challenge or the toolkit, feel free to open an issue on Github.

## Organizers
* [Goutam Bhat](https://goutamgmb.github.io/) (goutam.bhat@vision.ee.ethz.ch)
* [Martin Danelljan](https://martin-danelljan.github.io/) (martin.danelljan@vision.ee.ethz.ch)
* [Radu Timofte](http://people.ee.ethz.ch/~timofter/) (radu.timofte@vision.ee.ethz.ch)

## Terms and conditions
The terms and conditions for participating in the challenge are provided [here](terms_and_conditions.md)


## Acknowledgements
The toolkit uses the forward and inverse camera pipeline code from [unprocessing](https://github.com/timothybrooks/unprocessing).
